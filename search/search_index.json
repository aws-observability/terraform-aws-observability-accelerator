{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AWS Observability Accelerator for Terraform","text":"<p>Welcome to the AWS Observability Accelerator for Terraform!</p> <p>The AWS Observability Accelerator for Terraform is a set of opinionated modules to help you set up observability for your AWS environments with AWS-managed observability services such as Amazon Managed Service for Prometheus, Amazon Managed Grafana, AWS Distro for OpenTelemetry (ADOT) and Amazon CloudWatch.</p> <p>We provide curated metrics, logs, traces collection, alerting rules and Grafana dashboards for your AWS infrastructure and custom applications.</p> <p></p>"},{"location":"#getting-started","title":"Getting started","text":"<p>This project provides a set of Terraform modules to enable metrics, logs and traces collection, dashboards and alerts for monitoring:</p> <ul> <li> <p>EKS Monitoring with AWS-Managed Open Source - Get Prometheus metrics, CloudWatch logs collection, and X-Ray traces (with OTLP support) for your EKS cluster. Visualize key metrics and logs with provided Grafana dashboards and get pre-built alerting rules.</p> </li> <li> <p>EKS Monitoring with Enhanced CloudWatch Container Insights - Get deep visibility into EKS using Amazon CloudWatch for metrics collection, aggregation, and insights summaries. Includes support for CloudWatch Application Signals (preview).</p> </li> <li> <p>ECS Monitoring on EC2 with AWS-Managed Open Source - Collect metrics, traces, and logs for ECS on EC2 and send them to a Managed Prometheus workspace , X-Ray, and CloudWatch Logs. Includes pre-built Grafana dashboards for key metrics and logs.</p> </li> <li> <p>Managed Prometheus Monitoring - This module sets up automated observability for Amazon Managed Service for Prometheus workspaces, including a Grafana dashboard, CloudWatch monitoring, and service quota alarms.</p> </li> </ul> <p>These modules can be directly configured in your existing Terraform configurations or ready to be deployed in our packaged examples</p> <p>Tip</p> <p>We have supporting examples for quick setup such as:</p> <ul> <li>Creating a new Amazon EKS cluster and a VPC</li> <li>Creating and configure an Amazon Managed Grafana workspace with SSO</li> </ul>"},{"location":"#motivation","title":"Motivation","text":"<p>To gain deep visibility into your workloads and environments, AWS proposes a set of secure, scalable, highly available, production-grade managed open source services such as Amazon Managed Service for Prometheus, Amazon Managed Grafana and Amazon OpenSearch.</p> <p>AWS customers have asked for best-practices and guidance to collect metrics, logs and traces from their containerized applications and microservices with ease of deployment. Customers can use the AWS Observability Accelerator to configure their metrics and traces collection, leveraging AWS Distro for OpenTelemetry, to have opinionated dashboards and alerts available in only minutes.</p>"},{"location":"#support-feedback","title":"Support &amp; Feedback","text":"<p>AWS Observability Accelerator for Terraform is maintained by AWS Solution Architects. It is not part of an AWS service and support is provided best-effort by the AWS Observability Accelerator community.</p> <p>To post feedback, submit feature ideas, or report bugs, please use the issues section of this GitHub repo.</p> <p>If you are interested in contributing, see the contribution guide.</p>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#prerequisites","title":"Prerequisites","text":"<p>All examples in this repository require the following tools installed</p> <ol> <li>Terraform</li> <li>AWS CLI</li> <li>Kubectl</li> </ol>"},{"location":"concepts/#minimum-iam-policy","title":"Minimum IAM Policy","text":"<p>To run the examples, you need a set of AWS IAM permissions. You can find an example of minimum permissions required in this file.</p> <p>Warning</p> <p>The policy resource is set as <code>*</code> to allow all resources, this is not a recommended practice. You should restrict instead to the ARNs when applicable.</p>"},{"location":"concepts/#terraform-states-and-variables","title":"Terraform states and variables","text":"<p>By default, our examples are using local Terraform states. If you need your Terraform states to be saved remotely, on Amazon S3, visit the terraform remote states documentation.</p> <p>For simplicity, we use Terraform supported environment variables. You can also edit the <code>terraform.tfvars</code> files directly and deploy with <code>terraform apply -var-file=terraform.tfvars</code>. Terraform tfvars file can be useful if you need to track changes as part of a Git repository or CI/CD pipeline.</p> <p>Warning</p> <p>When using <code>tfvars</code> files, always be careful to not store and commit any secrets (keys,     passwords, ...)</p>"},{"location":"concepts/#grafana-contents-via-gitops-on-amazon-managed-grafana","title":"Grafana contents via GitOps on Amazon Managed Grafana","text":"<p>We have upgraded our solution to use grafana-operator and Flux to create Grafana data sources, folder and dashboards via GitOps on Amazon Managed Grafana.</p> <p>The grafana-operator is a Kubernetes operator built to help you manage your Grafana instances inside and outside Kubernetes. Grafana Operator makes it possible for you to manage and create Grafana dashboards, datasources etc. declaratively between multiple instances in an easy and scalable way. Using grafana-operator it will be possible to add AWS data sources such as Amazon Managed Service for Prometheus, Amazon CloudWatch, AWS X-Ray to Amazon Managed Grafana and create Grafana dashboards on Amazon Managed Grafana from your Amazon EKS cluster. This enables us to use our Kubernetes cluster to create and manage the lifecycle of resources in Amazon Managed Grafana in a Kubernetes native way. This ultimately enables us to use GitOps mechanisms using CNCF projects such as Flux  to create and manage the lifecycle of resources in Amazon Managed Grafana.</p> <p>GitOps is a way of managing application and infrastructure deployment so that the whole system is described declaratively in a Git repository. It is an operational model that offers you the ability to manage the state of multiple Kubernetes clusters leveraging the best practices of version control, immutable artifacts, and automation. Flux  is a declarative, GitOps-based continuous delivery tool that can be integrated into any CI/CD pipeline. It gives users the flexibility of choosing their Git provider (GitHub, GitLab, BitBucket). Now, with grafana-operator supporting the management of external Grafana instances such as Amazon Managed Grafana, operations personas can use GitOps mechanisms using CNCF projects such as Flux to create and manage the lifecycle of resources in Amazon Managed Grafana.</p> <p>We have setup a GitRepository and Kustomization using Flux to sync our GitHub Repository to add Grafana Datasources, folder and Dashboards to Amazon Managed Grafana using Grafana Operator. GitRepository defines a Source to produce an Artifact for a Git repository revision. Kustomization defines a pipeline for fetching, decrypting, building, validating and applying Kustomize overlays or plain Kubernetes manifests. we are also using Flux Post build variable substitution to dynamically render variables such as AMG_AWS_REGION, AMP_ENDPOINT_URL, AMG_ENDPOINT_URL,GRAFANA_NODEEXP_DASH_URL on the YAML manifests during deployment time to avoid hardcoding on the YAML manifests stored in Git repo.</p> <p>We have placed our declarative code snippet to create an Amazon Managed Service For Promethes datasource and Grafana Dashboard in Amazon Managed Grafana in our AWS Observabiity Accelerator GitHub Repository. We have setup a GitRepository to point to the AWS Observabiity Accelerator GitHub Repository and <code>Kustomization</code> for flux to sync Git Repository with artifacts in <code>./artifacts/grafana-operator-manifests/*</code> path in the AWS Observabiity Accelerator GitHub Repository. You can use this extension of our solution to point your own Kubernetes manifests to create Grafana Datasources and personified Grafana Dashboards of your choice using GitOps with Grafana Operator and Flux in Kubernetes native way with altering and redeploying this solution for changes to Grafana resources.</p>"},{"location":"concepts/#release-notes","title":"Release notes","text":"<p>We encourage you to use our release versions as much as possible to avoid breaking changes when deploying Terraform modules. You can read also our change log on the releases page. Here's an example of using a fixed version:</p> <pre><code>module \"eks_monitoring\" {\n    source = \"github.com/aws-observability/terraform-aws-observability-accelerator//modules/managed-prometheus-monitoring?ref=v2.5.0\"\n}\n</code></pre>"},{"location":"concepts/#modules","title":"Modules","text":"<p>Modules are set of functionalities (ex: Managed Open-Source EKS monitoring, CloudWatch Container Insights, ...) packaged together that can be used to add Observability to your environments. All the modules come with end-to-end deployable examples.</p>"},{"location":"concepts/#examples","title":"Examples","text":"<p>Examples put modules together in a ready to deploy terraform configuration as a starting point. With little to no configuration, you can run <code>terraform apply</code> and use the deployed resources on your AWS Account.</p> <p>You can find workload examples like Amazon EKS infrastructure monitoring or monitoring your Amazon Managed Service for Prometheus workspace and more.</p>"},{"location":"concepts/#getting-started-with-aws-observability-services","title":"Getting started with AWS Observability services","text":"<p>If you are new to AWS Observability services, or want to dive deeper into them, check our One Observability Workshop for a hands-on experience in a self-paced environment or at an AWS venue.</p>"},{"location":"contributors/","title":"Contributors","text":"<p>The content on this site is maintained by the Solutions Architects from the AWS observability team with support from the AWS service teams and other volunteers from across the organization.</p> <p>Our goal is to make it easier to use AWS Open Source Observability Services.</p> <p>The core team include the following people:</p> <ul> <li>Abhi Khanna</li> <li>Elamaran Shanmugam</li> <li>Imaya Kumar Jagannathan</li> <li>Jerome DECQ</li> <li>Kevin Lewin</li> <li>Michael Hausenblas</li> <li>Rodrigue Koffi</li> <li>Toshal Dudhwala</li> <li>Vikram Venkataraman</li> </ul> <p>We welcome the wider open source community and thank those who contribute to this project.</p> <p>Note that all information published on this site is available via the Apache 2.0 license.</p>"},{"location":"support/","title":"Support &amp; Feedback","text":"<p>AWS Observability Accelerator for Terraform is maintained by AWS Solution Architects. It is not part of an AWS service and support is provided best-effort by the AWS Observability Accelerator community.</p> <p>To post feedback, submit feature ideas, or report bugs, please use the issues section of this GitHub repo.</p> <p>If you are interested in contributing, see the contribution guide.</p>"},{"location":"adothealth/","title":"Monitoring ADOT collector health","text":"<p>The OpenTelemetry collector produces metrics to monitor the entire pipeline. In the EKS monitoring module, we have enabled those metrics by default with the AWS Distro for OpenTelemetry (ADOT) collector. You get a Grafana dashboard named <code>OpenTelemetry Health Collector</code>. This dashboard shows useful telemetry information about the ADOT collector itself which can be helpful when you want to troubleshoot any issues with the collector or understand how much resources the collector is consuming.</p> <p>Note</p> <p>The dashboard and metrics used are not specific to Amazon EKS, but applicable to any environment running an OpenTelemetry collector.</p> <p>Below diagram shows an example data flow and the components in an ADOT collector:</p> <p></p> <p>In this dashboard, there are five sections. Each section has metrics relevant to the various components of the AWS Distro for OpenTelemetry (ADOT) collector :</p>"},{"location":"adothealth/#receivers","title":"Receivers","text":"<p>Shows the receiver\u2019s accepted and refused rate/count of spans and metric points that are pushed into the telemetry pipeline.</p>"},{"location":"adothealth/#processors","title":"Processors","text":"<p>Shows the accepted and refused rate/count of spans and metric points pushed into next component in the pipeline. The batch metrics can help to understand how often metrics are sent to exporter and the batch size.</p> <p></p>"},{"location":"adothealth/#exporters","title":"Exporters","text":"<p>Shows the exporter\u2019s accepted and refused rate/count of spans and metric points that are pushed to any of the destinations. It also shows the size and capacity of the retry queue. These metrics can be used to understand if the collector is having issues in sending trace or metric data to the destination configured.</p> <p></p>"},{"location":"adothealth/#collectors","title":"Collectors","text":"<p>Shows the collector\u2019s operational metrics (Memory, CPU, uptime). This can be used to understand how much resources the collector is consuming.</p> <p></p>"},{"location":"adothealth/#data-flow","title":"Data Flow","text":"<p>Shows the metrics and spans data flow through the collector\u2019s components.</p> <p></p> <p>Note</p> <p>To read more about the metrics and the dashboard used, visit the upstream documentation here.</p>"},{"location":"adothealth/#deploy-instructions","title":"Deploy instructions","text":"<p>As this is enabled by default in the EKS monitoring module, visit this example\u2019s instructions which will provide the ADOT collector health dashboard after deployment</p>"},{"location":"adothealth/#disable-adot-health-monitoring","title":"Disable ADOT health monitoring","text":"<p>You can disable ADOT collector health metrics by setting the variable enable_adotcollector_metrics to false.</p> <pre><code>variable \"enable_adotcollector_metrics\" {\n  description = \"Enables collection of ADOT collector metrics\"\n  type        = bool\n  default     = true\n}\n</code></pre>"},{"location":"container-insights/eks/","title":"CloudWatch Application Signals &amp; Container Insights for your EKS Cluster","text":"<p>This example deploys CloudWatch Observability EKS add-on on an exisiting Amazon EKS cluster, which enables Container Insights enhanced observability for Amazon EKS and CloudWatch Application Signals by default.</p> <ol> <li>Enables the CloudWatch Observability Add-on on EKS using the IAM service account role</li> <li>Creates an IAM Service Linked role for enabling Application Signals</li> </ol>"},{"location":"container-insights/eks/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>Make sure to complete the prerequisites section before proceeding.</p>"},{"location":"container-insights/eks/#setup","title":"Setup","text":""},{"location":"container-insights/eks/#1-download-sources-and-initialize-terraform","title":"1. Download sources and initialize Terraform","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\ncd terraform-aws-observability-accelerator/examples/eks-container-insights\nterraform init\n</code></pre>"},{"location":"container-insights/eks/#2-aws-region","title":"2. AWS Region","text":"<p>Specify the AWS Region where the resources will be deployed:</p> <pre><code>export TF_VAR_aws_region=xxx\n</code></pre>"},{"location":"container-insights/eks/#2-eks-cluster-name","title":"2. EKS Cluster Name","text":"<p>Specify the EKS Cluster Name where the resources will be deployed:</p> <pre><code>export TF_VAR_eks_cluster_id=xxx\n</code></pre>"},{"location":"container-insights/eks/#3-disable-creation-of-cloudwatch-application-signals-service-linked-role","title":"3. Disable creation of <code>Cloudwatch Application Signals Service-linked Role</code>","text":"<p>If you already have Application Signals deployed in your AWS account, please set the value of this variable to <code>false</code> <pre><code>variable \"create_cloudwatch_application_signals_role\" {\n  type        = bool\n  default     = true\n  description = \"Create a Cloudwatch Application Signals service-linked role\"\n}\n</code></pre></p>"},{"location":"container-insights/eks/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy the example</p> <pre><code>terraform apply\n</code></pre>"},{"location":"container-insights/eks/#enabling-application-signals-for-your-services","title":"Enabling Application Signals for your services","text":"<p>Amazon CloudWatch Application Signals is a new integrated native APM experience in AWS. CloudWatch Application Signals supports Java and Python applications running on your Amazon EKS cluster.</p> <p>Next, you have to update your Application to <code>Configure application metrics and trace sampling</code>. For this, you must add an annotation to a manifest YAML in your cluster. Adding this annotation auto-instruments the application to send metrics, traces, and logs to Application Signals. You have two options for the annotation:</p> <ol> <li> <p>Annotate Workload auto-instruments a single workload in the cluster.</p> <ul> <li>Paste the below line into the PodTemplate section of the workload manifest. <pre><code>apiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    metadata:\n      # add this annotation under the pod template metadata of the services deployment YAML you want to monitor\n      annotations:\n        instrumentation.opentelemetry.io/inject-java: \"true\"\n        instrumentation.opentelemetry.io/inject-python: \"true\"\n...\n</code></pre></li> <li>In your terminal, enter <code>kubectl apply -f your_deployment_yaml</code> to apply the change.</li> </ul> </li> <li> <p>Annotate Namespace auto-instruments all workloads deployed in the selected namespace.</p> <ul> <li>Paste the below line into the metadata section of the namespace manifest. <pre><code>annotations: instrumentation.opentelemetry.io/inject-java: \"true\"\napiVersion: apps/v1\nkind: Namespace\nmetadata:\n    name: &lt;your_namespace&gt;\n    # add this annotation under metadata of the namespace manifest you want to monitor\n    annotations:\n      instrumentation.opentelemetry.io/inject-java: \"true\"\n      instrumentation.opentelemetry.io/inject-python: \"true\"\n...\n</code></pre></li> <li>In your terminal, enter <code>kubectl apply -f your_namespace_yaml</code> to apply the change.</li> <li>In your terminal, enter a command to restart all pods in the namespace. An example command to restart deployment workloads is <code>kubectl rollout restart deployment -n namespace_name</code></li> </ul> </li> </ol>"},{"location":"container-insights/eks/#visualization-of-container-insights-data","title":"Visualization of Container Insights data","text":"<p>After <code>terraform apply</code> is successful, open your Amazon CloudWatch console in the same region as your EKS cluster, then from the left hand side choose <code>Insights -&gt; Container Insights</code>, there choose the <code>EKS</code> from the drop down and you will see the metrics shown on the dashboard:</p> <p></p>"},{"location":"container-insights/eks/#visualization-of-cloudwatch-application-signals-data","title":"Visualization of CloudWatch Application Signals data","text":"<p>After enabling your Application to pass metrics and traces by following the steps provided above, open your Amazon CloudWatch console in the same region as your EKS cluster, then from the left hand side choose <code>Application Signals -&gt; Services</code> and you will see the metrics shown on the sample dashboard below:</p> <p></p> <p></p>"},{"location":"container-insights/eks/#cleanup","title":"Cleanup","text":"<p>To clean up your environment, destroy the Terraform example by running</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"ecs/ecs-monitoring-on-ec2/","title":"Amazon ECS on EC2 cluster monitoring","text":"<p>This example demonstrates how to monitor your Amazon Elastic Container Service on EC2 (Amazon ECS) cluster with the Observability Accelerator's ECS monitoring module</p> <p>The module collects Prometheus metrics from tasks running on ECS and sends it to Prometheus using AWS Distro for OpenTelemetry Collector (ADOT).</p> <p>You can either run the collector as a sidecar or deploy the collector as its own ECS service for entire cluster. ECS tasks with Prometheus endpoints are discovered using extension ecsobserver. (Unlike EKS, there is no builtin discovery for ECS inside prometheus)</p> <p>Additionally, you can optionally collect custom Prometheus metrics from your applications running on your ECS cluster.</p>"},{"location":"ecs/ecs-monitoring-on-ec2/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>Make sure to complete the prerequisites section before proceeding.</p>"},{"location":"ecs/ecs-monitoring-on-ec2/#available-samples-for-various-worklods","title":"Available Samples for various Worklods","text":"<p>Make sure to update your exisitng Application Task Definitions based on the workload type :-</p>"},{"location":"ecs/ecs-monitoring-on-ec2/#1-javajmx-workload-for-ecs-clusters","title":"1. Java/JMX workload for ECS Clusters","text":""},{"location":"ecs/ecs-monitoring-on-ec2/#2-nginx-workload-for-amazon-ecs-clusters","title":"2. NGINX workload for Amazon ECS clusters","text":""},{"location":"ecs/ecs-monitoring-on-ec2/#3-app-mesh-workload","title":"3. App Mesh workload","text":""},{"location":"ecs/ecs-monitoring-on-ec2/#setup","title":"Setup","text":""},{"location":"ecs/ecs-monitoring-on-ec2/#1-add-the-ecs-monitoring-module-to-your-exisitng-ecs-cluster","title":"1. Add the ECS Monitoring Module to your exisitng ECS Cluster","text":"<pre><code>module \"ecs_monitoring\" {\n  source                = \"../../modules/ecs-monitoring\"\n  aws_ecs_cluster_name  = module.ecs_cluster.cluster_name\n  task_role_arn           = module.ecs_cluster.task_exec_iam_role_arn\n  execution_role_arn      = module.ecs_cluster.task_exec_iam_role_arn\n\n  depends_on = [\n    module.ecs_cluster\n  ]\n}\n</code></pre>"},{"location":"ecs/ecs-monitoring-on-ec2/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy the example</p> <pre><code>terraform apply\n</code></pre>"},{"location":"ecs/ecs-monitoring-on-ec2/#visualization","title":"Visualization","text":""},{"location":"ecs/ecs-monitoring-on-ec2/#cleanup","title":"Cleanup","text":"<p>To clean up your environment, destroy the Terraform example by running</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"eks/","title":"Amazon EKS cluster metrics","text":"<p>This example demonstrates how to monitor your Amazon Elastic Kubernetes Service (Amazon EKS) cluster with the Observability Accelerator's EKS monitoring module.</p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p> <p>The Amazon EKS infrastructure Terraform modules focuses on metrics collection to Amazon Managed Service for Prometheus using the AWS Distro for OpenTelemetry Operator for Amazon EKS. It deploys the node exporter and kube-state-metrics in your cluster.</p> <p>It provides default dashboards to get a comprehensible visibility on your nodes, namespaces, pods, and Kubelet operations health. Finally, you get curated Prometheus recording rules and alerts to operate your cluster.</p> <p>Additionally, you can optionally collect custom Prometheus metrics from your applications running on your EKS cluster.</p>"},{"location":"eks/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>Make sure to complete the prerequisites section before proceeding.</p>"},{"location":"eks/#setup","title":"Setup","text":""},{"location":"eks/#1-download-sources-and-initialize-terraform","title":"1. Download sources and initialize Terraform","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\ncd examples/existing-cluster-with-base-and-infra\nterraform init\n</code></pre>"},{"location":"eks/#2-aws-region","title":"2. AWS Region","text":"<p>Specify the AWS Region where the resources will be deployed:</p> <pre><code>export TF_VAR_aws_region=xxx\n</code></pre>"},{"location":"eks/#3-amazon-eks-cluster","title":"3. Amazon EKS Cluster","text":"<p>To run this example, you need to provide your EKS cluster name. If you don't have a cluster ready, visit this example first to create a new one.</p> <p>Specify your cluster name:</p> <pre><code>export TF_VAR_eks_cluster_id=xxx\n</code></pre>"},{"location":"eks/#4-amazon-managed-service-for-prometheus-workspace-optional","title":"4. Amazon Managed Service for Prometheus workspace (optional)","text":"<p>By default, we create an Amazon Managed Service for Prometheus workspace for you. However, if you have an existing workspace you want to reuse, edit and run:</p> <pre><code>export TF_VAR_managed_prometheus_workspace_id=ws-xxx\n</code></pre> <p>To create a workspace outside of Terraform's state, simply run:</p> <pre><code>aws amp create-workspace --alias observability-accelerator --query '.workspaceId' --output text\n</code></pre>"},{"location":"eks/#5-amazon-managed-grafana-workspace","title":"5. Amazon Managed Grafana workspace","text":"<p>To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</p> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.eu-central-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export TF_VAR_managed_grafana_workspace_id=g-xxx\n</code></pre>"},{"location":"eks/#6-grafana-authentication","title":"6. Grafana authentication","text":"<p>Grafana Service Accounts and Service Account Tokens have been introduced in Amazon Managed Grafana v9.4, which replaces Grafana API Keys in v10.4. Amazon Managed Grafana provides new control plane APIs to automate their creation. If you are still using a workspace in Grafana v8.4, you can use a Grafana API Key.</p> <p>As a security best practice, we will provide Terraform a short lived token to run the <code>apply</code> or <code>destroy</code> command.</p> <p>Ensure you have necessary IAM permissions (<code>CreateWorkspaceServiceAccount, CreateWorkspaceServiceAccountToken, DeleteWorkspaceServiceAccounts, DeleteWorkspaceServiceAccountToken</code>) for Service Accounts and (<code>CreateWorkspaceApiKey, DeleteWorkspaceApiKey</code>) for Grafana API key.</p> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $TF_VAR_managed_grafana_workspace_id \\\n  --grafana-role ADMIN \\\n  --name terraform-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token for running Terraform\nexport TF_VAR_grafana_api_key=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $TF_VAR_managed_grafana_workspace_id \\\n  --name \"observability-accelerator-$(date +%s)\" \\\n  --seconds-to-live 7200 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export TF_VAR_grafana_api_key=`aws grafana create-workspace-api-key --key-name \"observability-accelerator-$(date +%s)\" --key-role ADMIN --seconds-to-live 7200 --workspace-id $TF_VAR_managed_grafana_workspace_id --query key --output text`\n</code></pre> <p>Note</p> <p>The <code>grafana_api_key</code> variable accepts both Grafana API key or a service account token</p>"},{"location":"eks/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy the example</p> <pre><code>terraform apply\n</code></pre>"},{"location":"eks/#visualization","title":"Visualization","text":""},{"location":"eks/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code> </p> <p>Open a specific dashboard and you should be able to view its visualization </p> <p>With v2.5 and above, the dashboards are managed with a Grafana Operator running in your cluster. From the cluster to view all dashboards as Kubernetes objects, run</p> <pre><code>kubectl get grafanadashboards -A\nNAMESPACE          NAME                                   AGE\ngrafana-operator   cluster-grafanadashboard               138m\ngrafana-operator   java-grafanadashboard                  143m\ngrafana-operator   kubelet-grafanadashboard               13h\ngrafana-operator   namespace-workloads-grafanadashboard   13h\ngrafana-operator   nginx-grafanadashboard                 134m\ngrafana-operator   node-exporter-grafanadashboard         13h\ngrafana-operator   nodes-grafanadashboard                 13h\ngrafana-operator   workloads-grafanadashboard             13h\n</code></pre> <p>You can inspect more details per dashboard using this command</p> <pre><code>kubectl describe grafanadashboards cluster-grafanadashboard -n grafana-operator\n</code></pre> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"eks/#3-amazon-managed-service-for-prometheus-rules-and-alerts","title":"3. Amazon Managed Service for Prometheus rules and alerts","text":"<p>Open the Amazon Managed Service for Prometheus console and view the details of your workspace. Under the <code>Rules management</code> tab, you should find new rules deployed.</p> <p></p> <p>Note</p> <p>To setup your alert receiver, with Amazon SNS, follow this documentation</p>"},{"location":"eks/#custom-prometheus-metrics-collection","title":"Custom Prometheus metrics collection","text":"<p>In addition to the cluster metrics, if you are interested in collecting Prometheus metrics from your pods, you can use setup <code>custom metrics collection</code>. This will instruct the ADOT collector to scrape your applications metrics based on the configuration you provide. You can also exclude some of the metrics and save costs.</p> <p>Using the example, you can edit <code>examples/existing-cluster-with-base-and-infra/main.tf</code>. In the module <code>module \"workloads_infra\" {</code> add the following config (make sure the values matches your use case):</p> <pre><code>enable_custom_metrics = true\n\ncustom_metrics_config = {\n    custom_app_1 = {\n        enableBasicAuth       = true\n        path                  = \"/metrics\"\n        basicAuthUsername     = \"username\"\n        basicAuthPassword     = \"password\"\n        ports                 = \".*:(8080)$\"\n        droppedSeriesPrefixes = \"(unspecified.*)$\"\n    }\n}\n</code></pre> <p>After applying Terraform, on Grafana, you can query Prometheus for your application metrics, create alerts and build on your own dashboards. On the explorer section of Grafana, the following query will give you the containers exposing metrics that matched the custom metrics collection, grouped by cluster and node.</p> <pre><code>sum(up{job=\"custom-metrics\"}) by (container_name, cluster, nodename)\n</code></pre> <p></p>"},{"location":"eks/destroy/","title":"Destroy resources","text":"<p>If you leave this stack running, you will continue to incur charges. To remove all resources created by Terraform, refresh your Grafana API key and run the command below.</p> <p>Warning</p> <p>Be careful, this command will removing everything created by Terraform. If you wish to keep your Amazon Managed Grafana or Amazon Managed Service for Prometheus workspaces. Remove them from your terraform state before running the destroy command.</p> <pre><code>terraform destroy\n</code></pre> <p>To remove resources from your Terraform state, run</p> <pre><code># prometheus workspace\nterraform state rm \"module.eks_observability_accelerator.aws_prometheus_workspace.this[0]\"\n</code></pre> <p>Note</p> <p>To view all the features proposed by this module, visit the module documentation.</p>"},{"location":"eks/eks-apiserver/","title":"Monitoring Amazon EKS API server","text":"<p>AWS Distro for OpenTelemetry (ADOT) enables Amazon EKS API server monitoring by default and provides three Grafana dashboards:</p>"},{"location":"eks/eks-apiserver/#kube-apiserver-basic","title":"Kube-apiserver (basic)","text":"<p>The basic dashboard shows metrics recommended in EKS Best Practices Guides - Monitor Control Plane Metrics and provides request rate and latency for API server, latency for ETCD server and overall workqueue service time and latency. It allows a drill-down per API server.</p> <p></p>"},{"location":"eks/eks-apiserver/#kube-apiserver-advanced","title":"Kube-apiserver (advanced)","text":"<p>The advanced dashboard is derived from kube-prometheus-stack <code>Kubernetes / API server</code> dashboard and provides a detailed metrics drill-down for example per READ and WRITE operations per component (like deployments, configmaps etc.).</p> <p></p>"},{"location":"eks/eks-apiserver/#kube-apiserver-troubleshooting","title":"Kube-apiserver (troubleshooting)","text":"<p>This dashboard can be used to troubleshoot API server problems like latency, errors etc.</p> <p>A detailed description for usage and background information regarding the dashboard can be found in AWS Containers blog post Troubleshooting Amazon EKS API servers with Prometheus.</p> <p></p>"},{"location":"eks/gpu-monitoring/","title":"Monitoring NVIDIA GPU Workloads","text":"<p>GPUs play an integral part in data intensive workloads. The eks-monitoring module of the Observability Accelerator provides the ability to deploy the NVIDIA DCGM Exporter Dashboard. The dashboard utilizes metrics scraped from the <code>/metrics</code> endpoint that are exposed when running the nvidia gpu operator with the DCGM exporter and NVSMI binary.</p> <p>Note</p> <p>In order to make use of this dashboard, you will need to have a GPU backed EKS cluster and deploy the GPU operator The recommended way of deploying the GPU operator is the Data on EKS Blueprint</p>"},{"location":"eks/gpu-monitoring/#deployment","title":"Deployment","text":"<p>This is enabled by default in the eks-monitoring module.</p>"},{"location":"eks/gpu-monitoring/#dashboards","title":"Dashboards","text":"<p>In order to start producing diagnostic metrics you must first deploy the nvidia SMI binary. nvidia-smi (also NVSMI) provides monitoring and management capabilities for each of NVIDIA\u2019s devices from Fermi and higher architecture families. We can now deploy the nvidia-smi binary, which shows diagnostic information about all GPUs visible to the container:</p> <p><pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: nvidia-smi\n    image: \"nvidia/cuda:11.0.3-base-ubuntu20.04\"\n    args:\n    - \"nvidia-smi\"\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre> After producing the metrics they should populate the DCGM exporter dashboard:</p> <p></p>"},{"location":"eks/istio/","title":"Monitor Istio running on Amazon EKS","text":"<p>This example demonstrates how to use Terraform modules for AWS Observability Accelerator, EKS Blueprints with the Tetrate Istio Add-on and EKS monitoring for Istio.</p> <p>The current example deploys the AWS Distro for OpenTelemetry Operator for Amazon EKS with its requirements and make use of an existing Amazon Managed Grafana workspace. It creates a new Amazon Managed Service for Prometheus workspace unless provided with an existing one to reuse.</p> <p>It uses the <code>EKS monitoring</code> module to provide an existing EKS cluster with an OpenTelemetry collector, curated Grafana dashboards, Prometheus alerting and recording rules with multiple configuration options for Istio.</p>"},{"location":"eks/istio/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have the following tools installed locally:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>terraform</li> <li>istioctl</li> </ol>"},{"location":"eks/istio/#setup","title":"Setup","text":"<p>This example uses a local terraform state. If you need states to be saved remotely, on Amazon S3 for example, visit the terraform remote states documentation</p>"},{"location":"eks/istio/#1-clone-the-repo-using-the-command-below","title":"1. Clone the repo using the command below","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\n</code></pre>"},{"location":"eks/istio/#2-initialize-terraform","title":"2. Initialize terraform","text":"<pre><code>cd examples/eks-istio\nterraform init\n</code></pre>"},{"location":"eks/istio/#3-amazon-eks-cluster","title":"3. Amazon EKS Cluster","text":"<p>To run this example, you need to provide your EKS cluster name. If you don't have a cluster ready, visit this example first to create a new one.</p> <p>Add your cluster name for <code>eks_cluster_id=\"...\"</code> to the <code>terraform.tfvars</code> or use an environment variable <code>export TF_VAR_eks_cluster_id=xxx</code>.</p>"},{"location":"eks/istio/#4-amazon-managed-grafana-workspace","title":"4. Amazon Managed Grafana workspace","text":"<p>To run this example you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable <code>export TF_VAR_managed_grafana_workspace_id=g-xxx</code>.</p> <p>To create a new one, visit this example.</p> <p>In the URL <code>https://g-xyz.grafana-workspace.eu-central-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p>"},{"location":"eks/istio/#5-grafana-api-key","title":"5.  Grafana API Key","text":"<p>Amazon Managed Service for Grafana provides a control plane API for generating Grafana API keys. We will provide to Terraform a short lived API key to run the <code>apply</code> or <code>destroy</code> command. Ensure you have necessary IAM permissions (<code>CreateWorkspaceApiKey, DeleteWorkspaceApiKey</code>)</p> <pre><code>export TF_VAR_grafana_api_key=`aws grafana create-workspace-api-key --key-name \"observability-accelerator-$(date +%s)\" --key-role ADMIN --seconds-to-live 1200 --workspace-id $TF_VAR_managed_grafana_workspace_id --query key --output text`\n</code></pre>"},{"location":"eks/istio/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy (if using a variable definition file)</p> <pre><code>terraform apply -var-file=terraform.tfvars\n</code></pre> <p>or if you had setup environment variables, run</p> <pre><code>terraform apply\n</code></pre>"},{"location":"eks/istio/#additional-configuration","title":"Additional configuration","text":"<p>For the purpose of the example, we have provided default values for some of the variables.</p> <ol> <li>AWS Region</li> </ol> <p>Specify the AWS Region where the resources will be deployed. Edit the <code>terraform.tfvars</code> file and modify <code>aws_region=\"...\"</code>. You can also use environement variables <code>export TF_VAR_aws_region=xxx</code>.</p> <ol> <li>Amazon Managed Service for Prometheus workspace</li> </ol> <p>If you have an existing workspace, add <code>managed_prometheus_workspace_id=ws-xxx</code> or use an environment variable <code>export TF_VAR_managed_prometheus_workspace_id=ws-xxx</code>.</p>"},{"location":"eks/istio/#visualization","title":"Visualization","text":""},{"location":"eks/istio/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Go to the Dashboards panel of your Grafana workspace. You will see a list of Istio dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open one of the Istio dasbhoards and you will be able to view its visualization</p> <p></p>"},{"location":"eks/istio/#2-amazon-managed-service-for-prometheus-rules-and-alerts","title":"2. Amazon Managed Service for Prometheus rules and alerts","text":"<p>Open the Amazon Managed Service for Prometheus console and view the details of your workspace. Under the <code>Rules management</code> tab, you will find new rules deployed.</p> <p></p> <p>Note</p> <p>To setup your alert receiver, with Amazon SNS, follow this documentation</p>"},{"location":"eks/istio/#deploy-an-example-application-to-visualize-metrics","title":"Deploy an example application to visualize metrics","text":"<p>In this section we will deploy Istio's Bookinfo sample application and extract metrics using the AWS OpenTelemetry collector. When downloading and configuring <code>istioctl</code>, there are samples included in the Istio package directory. The deployment files for Bookinfo are found in the <code>samples</code> folder. Additional details can be found on Istio's Getting Started documentation</p>"},{"location":"eks/istio/#1-deploy-the-bookinfo-application","title":"1. Deploy the Bookinfo Application","text":"<ol> <li>Using the AWS CLI, configure kubectl so you can connect to your EKS cluster. Update for your region and EKS cluster name <pre><code>aws eks update-kubeconfig --region &lt;enter-your-region&gt; --name &lt;cluster-name&gt;\n</code></pre></li> <li>Label the default namespace for automatic Istio sidecar injection <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre></li> <li>Navigate to the Istio folder location. For example, if using Istio v1.18.2 in Downloads folder: <pre><code>cd ~/Downloads/istio-1.18.2\n</code></pre></li> <li>Deploy the Bookinfo sample application <pre><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre></li> <li>Connect the Bookinfo application with the Istio gateway <pre><code>kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml\n</code></pre></li> <li>Validate that there are no issues with the Istio configuration <pre><code>istioctl analyze\n</code></pre></li> <li>Get the DNS name of the load balancer for the Istio gateway <pre><code>GATEWAY_URL=$(kubectl get svc istio-ingressgateway -n istio-system -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n</code></pre></li> </ol>"},{"location":"eks/istio/#2-generate-traffic-for-the-istio-bookinfo-sample-application","title":"2. Generate traffic for the Istio Bookinfo sample application","text":"<p>For the Bookinfo sample application, visit <code>http://$GATEWAY_URL/productpage</code> in your web browser. To see trace data, you must send requests to your service. The number of requests depends on Istio\u2019s sampling rate and can be configured using the Telemetry API. With the default sampling rate of 1%, you need to send at least 100 requests before the first trace is visible. To send a 100 requests to the productpage service, use the following command: <pre><code>for i in $(seq 1 100); do curl -s -o /dev/null \"http://$GATEWAY_URL/productpage\"; done\n</code></pre></p>"},{"location":"eks/istio/#3-explore-the-istio-dashboards","title":"3. Explore the Istio dashboards","text":"<p>Log back into your Amazon Managed Grafana workspace and navigate to the dashboard side panel. Click on the <code>Observability Accelerator Dashboards</code> folder and open the <code>Istio Service</code> Dashboard. Use the Service dropdown menu to select the <code>reviews.default.svc.cluster.local</code> service. This gives details about metrics for the service, client workloads (workloads that are calling this service), and service workloads (workloads that are providing this service).</p> <p>Explore the Istio Control Plane, Mesh, and Performance dashboards as well.</p>"},{"location":"eks/istio/#destroy","title":"Destroy","text":"<p>To teardown and remove the resources created in this example:</p> <pre><code>kubectl delete -f samples/bookinfo/networking/bookinfo-gateway.yaml\nkubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml\nterraform destroy\n</code></pre>"},{"location":"eks/java/","title":"Monitor Java/JMX applications running on Amazon EKS","text":"<p>Note</p> <p>Since v2.x, Java based applications monitoring on EKS has been merged within the eks-monitoring module to allow visibility both on the cluster and the workloads, #59.</p> <p>In addition to EKS infrastructure monitoring, the current example provides curated Grafana dashboards, Prometheus alerting and recording rules with multiple configuration options for Java based workloads on EKS.</p>"},{"location":"eks/java/#setup","title":"Setup","text":""},{"location":"eks/java/#1-add-java-metrics-dashboards-and-alerts","title":"1. Add Java metrics, dashboards and alerts","text":"<p>From the previous example's configuration, simply enable the Java pattern's flag.</p> <pre><code>module \"eks_monitoring\" {\n   ...\n   enable_java = true\n}\n</code></pre> <p>You can further customize the Java pattern by providing <code>java_config</code> options.</p>"},{"location":"eks/java/#2-grafana-api-key","title":"2. Grafana API key","text":"<p>Make sure to refresh your temporary Grafana API key</p> <pre><code>export TF_VAR_managed_grafana_workspace_id=g-xxx\nexport TF_VAR_grafana_api_key=`aws grafana create-workspace-api-key --key-name \"observability-accelerator-$(date +%s)\" --key-role ADMIN --seconds-to-live 7200 --workspace-id $TF_VAR_managed_grafana_workspace_id --query key --output text`\n</code></pre>"},{"location":"eks/java/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy.</p> <pre><code>terraform apply\n</code></pre> <p>Note</p> <p>To see the complete Java example, open the example on the repository</p>"},{"location":"eks/java/#visualization","title":"Visualization","text":""},{"location":"eks/java/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Go to the Dashboards panel of your Grafana workspace. There will be a folder called <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open the \"Java/JMX\" dashboard to view its visualization</p> <p></p>"},{"location":"eks/java/#2-amazon-managed-service-for-prometheus-rules-and-alerts","title":"2. Amazon Managed Service for Prometheus rules and alerts","text":"<p>Open the Amazon Managed Service for Prometheus console and view the details of your workspace. Under the <code>Rules management</code> tab, you will find new rules deployed.</p> <p></p> <p>Note</p> <p>To setup your alert receiver, with Amazon SNS, follow this documentation</p>"},{"location":"eks/java/#deploy-an-example-java-application","title":"Deploy an example Java application","text":"<p>In this section we will reuse an example from the AWS OpenTelemetry collector repository. For convenience, the steps can be found below.</p>"},{"location":"eks/java/#1-clone-repository","title":"1. Clone repository","text":"<pre><code>git clone https://github.com/aws-observability/aws-otel-test-framework\ncd aws-otel-test-framework/sample-apps/jmx/\n</code></pre>"},{"location":"eks/java/#2-authenticate-to-amazon-ecr","title":"2. Authenticate to Amazon ECR","text":"<pre><code>export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text`\nexport AWS_REGION={region}\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n</code></pre>"},{"location":"eks/java/#3-create-an-amazon-ecr-repository","title":"3. Create an Amazon ECR repository","text":"<pre><code>aws ecr create-repository --repository-name prometheus-sample-tomcat-jmx \\\n --image-scanning-configuration scanOnPush=true \\\n --region $AWS_REGION\n</code></pre>"},{"location":"eks/java/#4-build-docker-image-and-push-to-ecr","title":"4. Build Docker image and push to ECR.","text":"<pre><code>docker build -t $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest .\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest\n</code></pre>"},{"location":"eks/java/#5-install-sample-application","title":"5. Install sample application","text":"<pre><code>export SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample\ncurl https://raw.githubusercontent.com/aws-observability/aws-otel-test-framework/terraform/sample-apps/jmx/examples/prometheus-metrics-sample.yaml &gt; metrics-sample.yaml\nsed -i \"s/{{aws_account_id}}/$AWS_ACCOUNT_ID/g\" metrics-sample.yaml\nsed -i \"s/{{region}}/$AWS_REGION/g\" metrics-sample.yaml\nsed -i \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" metrics-sample.yaml\nkubectl apply -f metrics-sample.yaml\n</code></pre> <p>Verify that the sample application is running:</p> <pre><code>kubectl get pods -n $SAMPLE_TRAFFIC_NAMESPACE\n\nNAME                              READY   STATUS              RESTARTS   AGE\ntomcat-bad-traffic-generator      1/1     Running             0          11s\ntomcat-example-7958666589-2q755   0/1     ContainerCreating   0          11s\ntomcat-traffic-generator          1/1     Running             0          11s\n</code></pre>"},{"location":"eks/logs/","title":"Viewing Logs","text":"<p>By default, we deploy a FluentBit daemon set in the cluster to collect worker logs for all namespaces. Logs collection can be disabled with <code>enable_logs = false</code>. Logs are collected and exported to Amazon CloudWatch Logs, which enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service.</p> <p>Further configuration options are available in the module documentation. This guide shows how you can leverage CloudWatch Logs in Amazon Managed Grafana for your cluster and application logs.</p>"},{"location":"eks/logs/#using-cloudwatch-logs-as-data-source-in-grafana","title":"Using CloudWatch Logs as data source in Grafana","text":"<p>Follow the documentation to enable Amazon CloudWatch as a data source. Make sure to provide permissions.</p> <p>Tip</p> <p>If you created your workspace with our provided example, Amazon CloudWatch data source has already been setup for you.</p> <p>All logs are delivered in the following CloudWatch Log groups naming pattern: <code>/aws/eks/observability-accelerator/{cluster-name}/{namespace}</code>. Log streams follow <code>{container-name}.{pod-name}</code>. In Grafana, querying and analyzing logs is done with CloudWatch Logs Insights</p>"},{"location":"eks/logs/#example-adot-collector-logs","title":"Example - ADOT collector logs","text":"<p>Select one or many log groups and run the following query. The example below, queries AWS Distro for OpenTelemetry (ADOT) logs</p> <pre><code>fields @timestamp, log\n| order @timestamp desc\n| limit 100\n</code></pre> <p></p>"},{"location":"eks/logs/#example-using-time-series-visualizations","title":"Example - Using time series visualizations","text":"<p>CloudWatch Logs syntax provide powerful functions to extract data from your logs. The <code>stats()</code> function allows you to calculate aggregate statistics with log field values. This is useful to have visualization on non-metric data from your applications.</p> <p>In the example below, we use the following query to graph the number of metrics collected by the ADOT collector</p> <pre><code>fields @timestamp, log\n| parse log /\"#metrics\": (?&lt;metrics_count&gt;\\d+)}/\n| stats avg(metrics_count) by bin(5m)\n| limit 100\n</code></pre> <p>Tip</p> <p>You can add logs in your dashboards with logs panel types or time series depending on your query results type.</p> <p></p> <p>Warning</p> <p>Querying CloudWatch logs will incur costs per GB scanned. Use small time windows and limits in your queries. Checkout the CloudWatch pricing page for more infos.</p>"},{"location":"eks/multiaccount/","title":"AWS EKS Cross Account Observability","text":"<p>This example shows how to use the AWS Observability Accelerator, with two or more EKS clusters in multiple AWS accounts and verify the collected metrics from all the clusters in the dashboards of a common <code>Amazon Managed Grafana</code> workspace in a central monitoring account.</p>"},{"location":"eks/multiaccount/#prerequisites","title":"Prerequisites","text":""},{"location":"eks/multiaccount/#1-cross-account-iam-access","title":"1. Cross Account IAM access","text":"<p>In order to create/modify resources across multiple AWS accounts, this Terraform example implements the cross-account IAM role assumption. You will need separate IAM roles in all 3 AWS accounts, and each of these IAM roles should have the below specified trust-relationship so that your local AWS user/role will be able to assume them during the terraform execution.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"&lt;local-aws-user/role-arn&gt;\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {}\n        }\n    ]\n}\n</code></pre> <p>Note</p> <p>The IAM roles in Account 1 and Account 2 (EKS cluster accounts) should have permissions to perform kubernetes API operations against your EKS clusters. For more info, please review documentation for enabling IAM principal access to your clusters</p>"},{"location":"eks/multiaccount/#2-eks-clusters-in-multiple-aws-accounts","title":"2. EKS clusters in multiple AWS Accounts","text":"<p>Using the example eks-cluster-with-vpc, create two EKS clusters with the below names in two different AWS accounts:</p> <ol> <li> <p><code>eks-cluster-1</code> (Account 1)</p> </li> <li> <p><code>eks-cluster-2</code> (Account 2)</p> </li> </ol> <p>Update the cluster names and their corresponding region names in the <code>variables.tf</code> file along with the corresponding IAM role ARNs that can be assumed by terraform to perform cross-account API operations.</p>"},{"location":"eks/multiaccount/#3-amazon-managed-grafana-amg-workspace","title":"3. Amazon Managed Grafana (AMG) workspace","text":"<p>To run this example you need an existing Amazon Managed Grafana (AMG) workspace. If not, you can create a new AMG workspace by following the Getting Started with Amazon Managed Grafana documentation.</p> <p>Add the Grafana Workspace ID and its corresponding region name in the <code>variables.tf</code> file along with the corresponding IAM role ARN that can be assumed by terraform to perform cross-account API operations.</p> <p>Note</p> <p>You can obtain the AMG Workspace ID based on its URL. For the URL <code>https://g-xyz.grafana-workspace.eu-central-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p>"},{"location":"eks/multiaccount/#setup","title":"Setup","text":""},{"location":"eks/multiaccount/#1-download-sources-and-initialize-terraform","title":"1. Download sources and initialize Terraform","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\n\ncd terraform-aws-observability-accelerator/examples/eks-cross-account-with-central-amp\n\nterraform init\n</code></pre>"},{"location":"eks/multiaccount/#2-deploy","title":"2. Deploy","text":"<p>By looking at the <code>variables.tf</code>, you will notice there are two EKS clusters targeted for deployment by the names/ids:</p> <ol> <li> <p><code>eks-cluster-1</code></p> </li> <li> <p><code>eks-cluster-2</code></p> </li> </ol> <p>While installing the observability settings for the EKS cluster specified in variable <code>cluster_one.name</code>, Terraform also sets up:</p> <ul> <li> <p>Creates an <code>Amazon Managed Prometheus Workspace</code></p> </li> <li> <p>Dashboard folder and files in provided <code>Amazon Managed Grafana Workspace</code></p> </li> </ul> <p>Warning</p> <p>To override the defaults, create a <code>terraform.tfvars</code> and change the default values of the variables.</p> <p>Run the following command to deploy</p> <pre><code>terraform  apply  --auto-approve\n</code></pre>"},{"location":"eks/multiaccount/#verifying-multi-account-observability","title":"Verifying Multi Account Observability","text":"<p>One you have successfully run the above setup, you should be able to see dashboards similar to the images shown below in <code>Amazon Managed Grafana</code> workspace.</p> <p>You will notice that you are able to use the <code>cluster</code> dropdown to filter the dashboards to metrics collected from a specific EKS cluster.</p> <p> </p>"},{"location":"eks/multiaccount/#cleanup","title":"Cleanup","text":"<p>To clean up entirely, run the following command:</p> <pre><code>terraform  destroy  --auto-approve\n</code></pre>"},{"location":"eks/multicluster/","title":"AWS EKS Multicluster Observability (single AWS Account)","text":"<p>This example shows how to use the AWS Observability Accelerator, with more than one EKS cluster in a single account and visualize the collected metrics from all the clusters in the dashboards of a common <code>Amazon Managed Grafana</code> workspace.</p>"},{"location":"eks/multicluster/#prerequisites","title":"Prerequisites","text":""},{"location":"eks/multicluster/#1-eks-clusters","title":"1. EKS clusters","text":"<p>Using the example eks-cluster-with-vpc, create two EKS clusters with the names:</p> <ol> <li><code>eks-cluster-1</code></li> <li><code>eks-cluster-2</code></li> </ol>"},{"location":"eks/multicluster/#2-amazon-managed-service-for-prometheus-amp-workspace","title":"2. Amazon Managed Service for Prometheus (AMP) workspace","text":"<p>We recommend that you create a new AMP workspace. To do that you can run the following command.</p> <p>Ensure you have the following necessary IAM permissions</p> <ul> <li><code>aps.CreateWorkspace</code></li> </ul> <pre><code>export TF_VAR_managed_prometheus_workspace_id=$(aws amp create-workspace --alias observability-accelerator --query='workspaceId' --output text)\n</code></pre>"},{"location":"eks/multicluster/#3-amazon-managed-grafana-amg-workspace","title":"3. Amazon Managed Grafana (AMG) workspace","text":"<p>To run this example you need an AMG workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for managed Grafana.</p> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.eu-central-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export TF_VAR_managed_grafana_workspace_id=g-xxx\n</code></pre>"},{"location":"eks/multicluster/#4-grafana-api-key","title":"4. Grafana API Key","text":"<p>AMG provides a control plane API for generating Grafana API keys. As a security best practice, we will provide to Terraform a short lived API key to run the <code>apply</code> or <code>destroy</code> command.</p> <p>Ensure you have the following necessary IAM permissions</p> <ul> <li><code>grafana.CreateWorkspaceApiKey</code></li> <li><code>grafana.DeleteWorkspaceApiKey</code></li> </ul> <pre><code>export TF_VAR_grafana_api_key=`aws grafana create-workspace-api-key --key-name \"observability-accelerator-$(date +%s)\" --key-role ADMIN --seconds-to-live 7200 --workspace-id $TF_VAR_managed_grafana_workspace_id --query key --output text`\n</code></pre>"},{"location":"eks/multicluster/#setup","title":"Setup","text":""},{"location":"eks/multicluster/#1-download-sources-and-initialize-terraform","title":"1. Download sources and initialize Terraform","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\ncd terraform-aws-observability-accelerator/examples/eks-multicluster\nterraform init\n</code></pre>"},{"location":"eks/multicluster/#2-deploy","title":"2. Deploy","text":"<p>Verify by looking at the file <code>variables.tf</code> that there are two EKS clusters targeted for deployment by the names/ids:</p> <ol> <li><code>eks-cluster-1</code></li> <li><code>eks-cluster-2</code></li> </ol> <p>The difference in deployment between these clusters is that Terraform, when setting up the EKS cluster behind variable <code>eks_cluster_1_id</code> for observability, also sets up:</p> <ul> <li>Dashboard folder and files in Amazon Managed Grafana</li> <li>Prometheus and Java, alerting and recording rules in Amazon Managed Service for Prometheus</li> </ul> <p>Warning</p> <p>To override the defaults, create a <code>terraform.tfvars</code> and change the default values of the variables.</p> <p>Run the following command to deploy</p> <pre><code>terraform apply --auto-approve\n</code></pre>"},{"location":"eks/multicluster/#verifying-multicluster-observability","title":"Verifying Multicluster Observability","text":"<p>One you have successfully run the above setup, you should be able to see dashboards similar to the images shown below in <code>Amazon Managed Grafana</code> workspace.</p> <p>Note how you are able to use the <code>cluster</code> dropdown to filter the dashboards to metrics collected from a specific EKS cluster.</p> <p></p> <p></p>"},{"location":"eks/multicluster/#cleanup","title":"Cleanup","text":"<p>To clean up entirely, run the following command:</p> <pre><code>terraform destroy --auto-approve\n</code></pre>"},{"location":"eks/nginx/","title":"Monitor Nginx applications running on Amazon EKS","text":"<p>Note</p> <p>Since v2.x, NGINX based applications monitoring on EKS has been merged within the eks-monitoring module to allow visibility both on the cluster and the workloads, #59.</p> <p>In addition to EKS infrastructure monitoring, the current example provides curated Grafana dashboards, Prometheus alerting and recording rules with multiple configuration options for NGINX based workloads on EKS.</p>"},{"location":"eks/nginx/#setup","title":"Setup","text":""},{"location":"eks/nginx/#1-add-nginx-metrics-dashboards-and-alerts","title":"1. Add NGINX metrics, dashboards and alerts","text":"<p>From the EKS cluster monitoring example's configuration, simply enable the NGINX pattern's flag.</p> <pre><code>module \"eks_monitoring\" {\n   ...\n   enable_nginx = true\n}\n</code></pre> <p>You can further customize the NGINX pattern by providing <code>nginx_config</code> options.</p>"},{"location":"eks/nginx/#2-grafana-api-key","title":"2. Grafana API key","text":"<p>Make sure to refresh your temporary Grafana API key</p> <pre><code>export TF_VAR_managed_grafana_workspace_id=g-xxx\nexport TF_VAR_grafana_api_key=`aws grafana create-workspace-api-key --key-name \"observability-accelerator-$(date +%s)\" --key-role ADMIN --seconds-to-live 1200 --workspace-id $TF_VAR_managed_grafana_workspace_id --query key --output text`\n</code></pre>"},{"location":"eks/nginx/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy.</p> <pre><code>terraform apply\n</code></pre> <p>Note</p> <p>To see the complete NGINX example, open the example on the repository</p>"},{"location":"eks/nginx/#visualization","title":"Visualization","text":"<ol> <li>Grafana dashboards</li> </ol> <p>Go to the Dashboards panel of your Grafana workspace. You will see a list of dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open the NGINX dashboard and you will be able to view its visualization</p> <p></p> <ol> <li>Amazon Managed Service for Prometheus rules and alerts</li> </ol> <p>Open the Amazon Managed Service for Prometheus console and view the details of your workspace. Under the <code>Rules management</code> tab, you will find new rules deployed.</p> <p></p> <p>Note</p> <p>To setup your alert receiver, with Amazon SNS, follow this documentation</p>"},{"location":"eks/nginx/#deploy-an-example-application-to-visualize-metrics","title":"Deploy an example application to visualize metrics","text":"<p>In this section we will deploy sample application and extract metrics using AWS OpenTelemetry collector</p>"},{"location":"eks/nginx/#1-add-the-helm-incubator-repo","title":"1. Add the helm incubator repo:","text":"<pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n</code></pre>"},{"location":"eks/nginx/#2-enter-the-following-command-to-create-a-new-namespace","title":"2. Enter the following command to create a new namespace:","text":"<pre><code>kubectl create namespace nginx-ingress-sample\n</code></pre>"},{"location":"eks/nginx/#3-enter-the-following-commands-to-install-nginx","title":"3. Enter the following commands to install NGINX:","text":"<pre><code>helm install my-nginx ingress-nginx/ingress-nginx \\\n--namespace nginx-ingress-sample \\\n--set controller.metrics.enabled=true \\\n--set-string controller.metrics.service.annotations.\"prometheus\\.io/port\"=\"10254\" \\\n--set-string controller.metrics.service.annotations.\"prometheus\\.io/scrape\"=\"true\"\n</code></pre>"},{"location":"eks/nginx/#4-set-an-external-ip-variable-to-the-value-of-the-external-ip-column-in-the-row-of-the-nginx-ingress-controller","title":"4. Set an EXTERNAL-IP variable to the value of the EXTERNAL-IP column in the row of the NGINX ingress controller.","text":"<pre><code>EXTERNAL_IP=your-nginx-controller-external-ip\n</code></pre>"},{"location":"eks/nginx/#5-start-some-sample-nginx-traffic-by-entering-the-following-command","title":"5. Start some sample NGINX traffic by entering the following command.","text":"<pre><code>SAMPLE_TRAFFIC_NAMESPACE=nginx-sample-traffic\ncurl https://raw.githubusercontent.com/aws-observability/terraform-aws-observability-accelerator/main/examples/existing-cluster-nginx/sample_traffic/nginx-traffic-sample.yaml |\nsed \"s/{{external_ip}}/$EXTERNAL_IP/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" |\nkubectl apply -f -\n</code></pre>"},{"location":"eks/nginx/#6-verify-if-the-application-is-running","title":"6. Verify if the application is running","text":"<pre><code>kubectl get pods -n nginx-ingress-sample\n</code></pre>"},{"location":"eks/nginx/#7-visualize-the-applications-dashboard","title":"7. Visualize the Application's dashboard","text":"<p>Log back into your Managed Grafana Workspace and navigate to the dashboard side panel, click on <code>Observability Accelerator Dashboards</code> Folder and open the <code>NGINX</code> Dashboard.</p>"},{"location":"eks/tracing/","title":"Tracing on Amazon EKS","text":"<p>Distributed tracing helps you have end-to-end visibility between transactions in distributed nodes. The <code>eks-monitoring</code> module is configured  by default to collect traces into AWS X-Ray.</p> <p>The AWS Distro for OpenTelemetry collector is configured to receive traces in the OTLP format (OTLP receiver), using the OpenTelemetry SDK or auto-instrumentation agents.</p> <p>Note</p> <p>To disable the tracing configuration, set up <code>enable_tracing = false</code> in the module configuration</p>"},{"location":"eks/tracing/#instrumentation","title":"Instrumentation","text":"<p>Let's take a sample application that is already instrumented with the OpenTelemetry SDK.</p> <p>Note</p> <p>To learn more about instrumenting with OpenTelemetry, please visit the OpenTelemetry documentation for your programming language.</p> <p>Cloning the repo</p> <pre><code>git clone https://github.com/aws-observability/aws-otel-community.git\ncd aws-otel-community/sample-apps/go-sample-app\n</code></pre>"},{"location":"eks/tracing/#deploying-on-amazon-eks","title":"Deploying on Amazon EKS","text":"<p>Using the sample application, we will build a container image, create and push an image on Amazon ECR. We will use a Kubernetes manifest to deploy to an EKS cluster.</p> <p>Warning</p> <p>The following steps require that you have an EKS cluster ready. To deploy an EKS cluster, please visit our example.</p>"},{"location":"eks/tracing/#building-container-image","title":"Building container image","text":"amd64 linuxcross platform build <pre><code>docker build -t go-sample-app .\n</code></pre> <pre><code>docker buildx build -t go-sample-app . --platform=linux/amd64\n</code></pre>"},{"location":"eks/tracing/#publishing-on-amazon-ecr","title":"Publishing on Amazon ECR","text":"using docker <pre><code>export ECR_REPOSITORY_URI=$(aws ecr create-repository --repository go-sample-app --query repository.repositoryUri --output text)\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_REPOSITORY_URI\ndocker tag go-sample-app:latest \"${ECR_REPOSITORY_URI}:latest\"\ndocker push \"${ECR_REPOSITORY_URI}:latest\"\n</code></pre>"},{"location":"eks/tracing/#deploying-on-amazon-eks_1","title":"Deploying on Amazon EKS","text":"eks.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: go-sample-app\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: go-sample-app\n  template:\n    metadata:\n      labels:\n        app: go-sample-app\n    spec:\n      containers:\n        - name: go-sample-app\n          image: \"${ECR_REPOSITORY_URI}:latest\" # make sure to replace this variable\n          imagePullPolicy: Always\n          env:\n          - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\n            value: adot-collector.adot-collector-kubeprometheus.svc.cluster.local:4317\n          resources:\n            limits:\n              cpu:  300m\n              memory: 300Mi\n            requests:\n              cpu: 100m\n              memory: 180Mi\n          ports:\n            - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: go-sample-app\n  namespace: default\n  labels:\n    app: go-sample-app\nspec:\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: go-sample-app\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: go-sample-app\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: go-sample-app\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n</code></pre>"},{"location":"eks/tracing/#deploying-and-testing","title":"Deploying and testing","text":"<p>With the Kubernetes manifest ready, run:</p> <pre><code>kubectl apply -f eks.yaml\n</code></pre> <p>You should see the pods running with the command:</p> <pre><code>kubectl get pods\nNAME                              READY   STATUS    RESTARTS        AGE\ngo-sample-app-67c48ff8c6-bdw74    1/1     Running   0               4s\ngo-sample-app-67c48ff8c6-t6k2j    1/1     Running   0               4s\n</code></pre> <p>To simulate some traffic you can forward the service port to your local host and test a few queries</p> <pre><code>kubectl port-forward deployment/go-sample-app 8080:8080\n</code></pre> <p>Test a few endpoints</p> <pre><code>curl http://localhost:8080/\ncurl http://localhost:8080/outgoing-http-call\ncurl http://localhost:8080/aws-sdk-call\ncurl http://localhost:8080/outgoing-sampleapp\n</code></pre>"},{"location":"eks/tracing/#visualizing-traces","title":"Visualizing traces","text":"<p>As this is a basic example, the service map doesn't have a lot of nodes, but this shows you how to setup tracing in your application and deploying it on Amazon EKS using the <code>eks-monitoring</code> module.</p> <p>With Flux and Grafana Operator, the <code>eks-monitoring</code> module configures an AWS X-Ray data source on your provided Grafana workspace. Open the Grafana explorer view and select the X-Ray data source. If you type the query below, and select <code>Trace List</code> for Query Type, you should see the list of traces occured in the selected timeframe.</p> <p></p> <p>You can add the service map to a dashboard, for example a service focused dashboard. You can click on any of the traces to view a node map and the traces details.</p> <p>There is a button that can take you the CloudWatch console to view the same data. If your logs are stored on CloudWatch Logs, this page can present all the logs in the trace details page. The CloudWatch Log Group name should be added to the trace as an attribute. Read more about this in our One Observability Workshop</p> <p></p>"},{"location":"eks/tracing/#resoures","title":"Resoures","text":"<ul> <li>AWS Observability Best Practices</li> <li>One Observability Workshop</li> <li>AWS Distro for OpenTelemetry documentation</li> <li>AWS X-Ray user guide</li> <li>OpenTelemetry documentation</li> </ul>"},{"location":"eks/troubleshooting/","title":"Troubleshooting guide for Amazon EKS monitoring module","text":"<p>Depending on your setup, you might face a few errors. If you encounter an error not listed here, please open an issue in the issues section</p> <p>These guide applies to the eks-monitoring Terraform module</p>"},{"location":"eks/troubleshooting/#cluster-authentication-issue","title":"Cluster authentication issue","text":""},{"location":"eks/troubleshooting/#error-message","title":"Error message","text":"<pre><code>\u2577\n\u2502 Error: cluster-secretstore-sm failed to create kubernetes rest client for update of resource: Get \"https://FINGERPRINT.gr7.us-east-1.eks.amazonaws.com/api?timeout=32s\": dial tcp: lookup F867DE6CE883F9595FC8A73D84FB9F83.gr7.us-east-1.eks.amazonaws.com on 192.168.4.1:53: no such host\n\u2502\n\u2502   with module.eks_monitoring.module.external_secrets[0].kubectl_manifest.cluster_secretstore,\n\u2502   on ../../modules/eks-monitoring/add-ons/external-secrets/main.tf line 59, in resource \"kubectl_manifest\" \"cluster_secretstore\":\n\u2502   59: resource \"kubectl_manifest\" \"cluster_secretstore\" {\n\u2502\n\u2575\n\u2577\n\u2502 Error: grafana-operator/external-secrets-sm failed to create kubernetes rest client for update of resource: Get \"https://FINGERPRINT.gr7.us-east-1.eks.amazonaws.com/api?timeout=32s\": dial tcp: lookup F867DE6CE883F9595FC8A73D84FB9F83.gr7.us-east-1.eks.amazonaws.com on 192.168.4.1:53: no such host\n\u2502\n\u2502   with module.eks_monitoring.module.external_secrets[0].kubectl_manifest.secret,\n\u2502   on ../../modules/eks-monitoring/add-ons/external-secrets/main.tf line 89, in resource \"kubectl_manifest\" \"secret\":\n\u2502   89: resource \"kubectl_manifest\" \"secret\" {\n</code></pre>"},{"location":"eks/troubleshooting/#resolution","title":"Resolution","text":"<p>To provision the <code>eks-monitoring</code> module, the environment where you are running Terraform apply needs to be authenticated against your cluster and be your current context. To verify, you can run a single <code>kubectl get nodes</code> command to ensure you are using the correct Amazon EKS cluster.</p> <p>To login agains the correct cluster, run:</p> <pre><code>aws eks update-kubeconfig --name &lt;cluster name&gt; --region &lt;aws region&gt;\n</code></pre>"},{"location":"eks/troubleshooting/#missing-grafana-dashboards","title":"Missing Grafana dashboards","text":"<p>Terraform apply can run without apparent errors and your Grafana workspace won't present any dashboards. Many situations could lead to this as described below. The best place to start would be checking the logs of <code>grafana-operator</code>, <code>external-secrets</code> and <code>flux-system</code> pods.</p>"},{"location":"eks/troubleshooting/#wrong-grafana-workspace","title":"Wrong Grafana workspace","text":"<p>It might happen that you provide the wrong Grafana workspace. One way to verify this is to run the following command:</p> <pre><code>kubectl describe grafanas external-grafana -n grafana-operator\n</code></pre> <p>You should see an output similar to this (truncated for brevity). Validate that you have the correct URL. If that's the case, re-running Terraform with the correct workspace ID, API key should fix this issue.</p> <pre><code>...\nSpec:\n  External:\n    API Key:\n      Key:   GF_SECURITY_ADMIN_APIKEY\n      Name:  grafana-admin-credentials\n    URL:     https://g-workspaceid.grafana-workspace.eu-central-1.amazonaws.com\nStatus:\n  Admin URL:  https://g-workspaceid.grafana-workspace.eu-central-1.amazonaws.com\n  Dashboards:\n    grafana-operator/apiserver-troubleshooting-grafanadashboard/V3y_Zcb7k\n    grafana-operator/apiserver-basic-grafanadashboard/R6abPf9Zz\n    grafana-operator/java-grafanadashboard/m9mHfAy7ks\n    grafana-operator/grafana-dashboards-adothealth/reshmanat\n    grafana-operator/apiserver-advanced-grafanadashboard/09ec8aa1e996d6ffcd6817bbaff4db1b\n    grafana-operator/nginx-grafanadashboard/nginx\n    grafana-operator/kubelet-grafanadashboard/3138fa155d5915769fbded898ac09fd9\n    grafana-operator/cluster-grafanadashboard/efa86fd1d0c121a26444b636a3f509a8\n    grafana-operator/workloads-grafanadashboard/a164a7f0339f99e89cea5cb47e9be617\n    grafana-operator/grafana-dashboards-kubeproxy/632e265de029684c40b21cb76bca4f94\n    grafana-operator/nodes-grafanadashboard/200ac8fdbfbb74b39aff88118e4d1c2c\n    grafana-operator/node-exporter-grafanadashboard/v8yDYJqnz\n    grafana-operator/namespace-workloads-grafanadashboard/a87fb0d919ec0ea5f6543124e16c42a5\n</code></pre>"},{"location":"eks/troubleshooting/#grafana-api-key-expired","title":"Grafana API key expired","text":"<p>Check on the logs on your grafana operator pod using the below command :</p> <pre><code>kubectl get pods -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ngrafana-operator-866d4446bb-nqq5c   1/1     Running   0          3h17m\n</code></pre> <pre><code>kubectl logs grafana-operator-866d4446bb-nqq5c -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>1.6857285045556655e+09  ERROR   error reconciling datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"grafana-operator\"}, \"namespace\": \"grafana-operator\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"72cfd60c-a255-44a1-bfbd-88b0cbc4f90c\", \"datasource\": \"grafanadatasource-sample-amp\", \"grafana\": \"external-grafana\", \"error\": \"status: 401, body: {\\\"message\\\":\\\"Expired API key\\\"}\\n\"}\ngithub.com/grafana-operator/grafana-operator/controllers.(*GrafanaDatasourceReconciler).Reconcile\n</code></pre> <p>If you observe, the the above <code>grafana-api-key error</code> in the logs, your grafana API key is expired.</p> <p>Please use the operational procedure to update your <code>grafana-api-key</code> :</p> <ul> <li> <p>Create a new Grafana API key, you can use this step and make sure the API key duration is not too short.</p> </li> <li> <p>Run Terraform with the new API key. Terraform will modify the AWS SSM Parameter used by <code>externalsecret</code>.</p> </li> <li> <p>If the issue persists, you can force the synchronization by deleting the <code>externalsecret</code> Kubernetes object.</p> </li> </ul> <pre><code>kubectl delete externalsecret/external-secrets-sm -n grafana-operator\n</code></pre>"},{"location":"eks/troubleshooting/#git-repository-errors","title":"Git repository errors","text":"<p>Flux is responsible to regularly pull and synchronize dashboards and artifacts into your EKS cluster. It might happen that its state gets corrupted.</p> <p>You can verify those errors by using this command. You should see an error if Flux is not able to pull correctly:</p> <pre><code>kubectl get gitrepositories -n flux-system\nNAME                            URL                                                                  AGE     READY   STATUS\naws-observability-accelerator   https://github.com/aws-observability/aws-observability-accelerator   6d12h   True    stored artifact for revision 'v0.2.0@sha1:c4819a990312f7c2597f529577471320e5c4ef7d'\n</code></pre> <p>Depending on the error, you can delete the repository and re-run Terraform and force the synchronization.</p> <pre><code>k delete gitrepositories aws-observability-accelerator  -n flux-system\n</code></pre> <p>If you believe this is a bug, please open an issue here.</p>"},{"location":"eks/troubleshooting/#flux-kustomizations","title":"Flux Kustomizations","text":"<p>After Flux pulls the repository in the cluster state, it will apply Kustomizations to create Grafana data sources, folders and dashboards.</p> <ul> <li>Check the kustomization objects. Here you should see the dashboards you have enabled</li> </ul> <pre><code>k get kustomizations.kustomize.toolkit.fluxcd.io -A\nNAMESPACE     NAME                                AGE   READY   STATUS\nflux-system   grafana-dashboards-adothealth       18d   True    Applied revision: v0.2.0@sha1:c4819a990312f7c2597f529577471320e5c4ef7d\nflux-system   grafana-dashboards-apiserver        18d   True    Applied revision: v0.2.0@sha1:c4819a990312f7c2597f529577471320e5c4ef7d\nflux-system   grafana-dashboards-infrastructure   10d   True    Applied revision: v0.2.0@sha1:c4819a990312f7c2597f529577471320e5c4ef7d\nflux-system   grafana-dashboards-java             18d   True    Applied revision: v0.2.0@sha1:c4819a990312f7c2597f529577471320e5c4ef7d\nflux-system   grafana-dashboards-kubeproxy        10d   True    Applied revision: v0.2.0@sha1:c4819a990312f7c2597f529577471320e5c4ef7d\nflux-system   grafana-dashboards-nginx            18d   True    Applied revision: v0.2.0@sha1:c4819a990312f7c2597f529577471320e5c4ef7d\n</code></pre> <ul> <li>To have more infos on an error, you can view the Kustomization controller logs</li> </ul> <pre><code>kubectl get pods -n flux-system\nNAME                                          READY   STATUS    RESTARTS      AGE\nhelm-controller-65cc46469f-nsqd5              1/1     Running   2 (13d ago)   27d\nimage-automation-controller-d8f7bfcb4-k2m9j   1/1     Running   2 (13d ago)   27d\nimage-reflector-controller-68979dfd49-wh25h   1/1     Running   2 (13d ago)   27d\nkustomize-controller-767677f7f5-c5xsp         1/1     Running   5 (13d ago)   63d\nnotification-controller-55d8c759f5-7df5l      1/1     Running   5 (13d ago)   63d\nsource-controller-58c66d55cd-4j6bl            1/1     Running   5 (13d ago)   63d\n</code></pre> <pre><code>kubectl logs -f -n flux-system kustomize-controller-767677f7f5-c5xsp\n</code></pre> <p>If you believe there is a bug, please open an issue here.</p> <ul> <li>Depending on the error, delete the kustomization object and re-apply Terraform</li> </ul> <pre><code>kubectl delete kustomizations -n flux-system grafana-dashboards-apiserver\n</code></pre>"},{"location":"eks/troubleshooting/#grafana-dashboards-errors","title":"Grafana dashboards errors","text":"<p>If all of the above seem normal, finally inspect deployed dashboards by running this command:</p> <pre><code>kubectl get grafanadashboards -A\nNAMESPACE          NAME                                         AGE\ngrafana-operator   apiserver-advanced-grafanadashboard          18d\ngrafana-operator   apiserver-basic-grafanadashboard             18d\ngrafana-operator   apiserver-troubleshooting-grafanadashboard   18d\ngrafana-operator   cluster-grafanadashboard                     10d\ngrafana-operator   grafana-dashboards-adothealth                18d\ngrafana-operator   grafana-dashboards-kubeproxy                 10d\ngrafana-operator   java-grafanadashboard                        18d\ngrafana-operator   kubelet-grafanadashboard                     10d\ngrafana-operator   namespace-workloads-grafanadashboard         10d\ngrafana-operator   nginx-grafanadashboard                       18d\ngrafana-operator   node-exporter-grafanadashboard               10d\ngrafana-operator   nodes-grafanadashboard                       10d\ngrafana-operator   workloads-grafanadashboard                   10d\n</code></pre> <ul> <li>You can dive into the details of a dashboard by running:</li> </ul> <pre><code>kubectl describe grafanadashboards grafana-dashboards-kubeproxy -n grafana-operator\n</code></pre> <ul> <li>Depending on the error, you can delete the dashboard object. In this case, you don't need to re-run Terraform as the Flux Kustomization will force its recreation through the Grafana operator</li> </ul> <pre><code>kubectl describe grafanadashboards grafana-dashboards-kubeproxy -n grafana-operator\n</code></pre> <p>If you believe there is a bug, please open an issue here.</p>"},{"location":"eks/troubleshooting/#upgrade-from-to-v25-or-earlier","title":"Upgrade from to v2.5 or earlier","text":"<p>v2.5.0 removes the dependency to the Terraform Grafana provider in the EKS monitoring module. As Grafana Operator manages and syncs the Grafana contents, Terraform is not required anymore in this context.</p> <p>However, if you migrate from earlier versions, you might leave some data orphan as the Grafana provider is dropped. Terraform will throw an error. We have released v2.5.0-rc.1 which removes all the Grafana resources provisioned by Terraform in the EKS context, without removing the provider configurations.</p> <ul> <li>Step 1: migrate to v2.5.0-rc.1 and run apply</li> <li>Step 2: migrate to v2.5.0 or above</li> </ul>"},{"location":"helpers/ecs-cluster-with-vpc/","title":"Example Amazon ECS Cluster with VPC","text":"<p>This example deploys an AWS ECS Cluster with VPC and also add the ECS Monitoring module</p>"},{"location":"helpers/ecs-cluster-with-vpc/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>Make sure to complete the prerequisites section before proceeding.</p>"},{"location":"helpers/ecs-cluster-with-vpc/#setup","title":"Setup","text":""},{"location":"helpers/ecs-cluster-with-vpc/#1-download-sources-and-initialize-terraform","title":"1. Download sources and initialize Terraform\u00b6","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\ncd terraform-aws-observability-accelerator/examples/ecs-cluster-with-vpc\nterraform init\n</code></pre>"},{"location":"helpers/ecs-cluster-with-vpc/#2-aws-region","title":"2. AWS Region\u00b6","text":"<p>Specify the AWS Region where the resources will be deployed:</p> <pre><code>export TF_VAR_aws_region=xxx\n</code></pre>"},{"location":"helpers/ecs-cluster-with-vpc/#3-terraform-plan-to-validate-the-changesupdates","title":"3. Terraform Plan to validate the changes/updates","text":"<pre><code>terraform plan\n</code></pre>"},{"location":"helpers/ecs-cluster-with-vpc/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy the example</p> <pre><code>terraform apply\n</code></pre>"},{"location":"helpers/ecs-cluster-with-vpc/#cleanup","title":"Cleanup","text":"<p>To clean up your environment, destroy the Terraform example by running</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"helpers/managed-grafana/","title":"Creating a new Amazon Managed Grafana Workspace","text":"<p>This example creates an Amazon Managed Grafana Workspace with Amazon CloudWatch, AWS X-Ray and Amazon Managed Service for Prometheus datasources.</p> <p>The authentication method chosen for this example is with IAM Identity Center (former SSO). You can extend this example to add SAML.</p>"},{"location":"helpers/managed-grafana/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>Make sure to complete the prerequisites section before proceeding.</p>"},{"location":"helpers/managed-grafana/#setup","title":"Setup","text":""},{"location":"helpers/managed-grafana/#1-download-sources-and-initialize-terraform","title":"1. Download sources and initialize Terraform","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\ncd terraform-aws-observability-accelerator/examples/managed-grafana-workspace\nterraform init\n</code></pre>"},{"location":"helpers/managed-grafana/#2-aws-region","title":"2. AWS Region","text":"<p>Specify the AWS Region where the resources will be deployed:</p> <pre><code>export TF_VAR_aws_region=xxx\n</code></pre>"},{"location":"helpers/managed-grafana/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy the example</p> <pre><code>terraform apply\n</code></pre>"},{"location":"helpers/managed-grafana/#authentication","title":"Authentication","text":"<p>After apply, Terraform will output the Worksapce's URL, but you need to:</p> <ul> <li>Setup user(s) in the IAM Identity Center (former SSO)</li> <li>Assign the user(s) to the workspace with proper permissions</li> </ul> <p></p>"},{"location":"helpers/managed-grafana/#cleanup","title":"Cleanup","text":"<p>To clean up your environment, destroy the Terraform example by running</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"helpers/new-eks-cluster/","title":"Creating a new Amazon EKS cluster with VPC","text":"<p>This example deploys the following:</p> <ul> <li>New sample VPC, 3 Private Subnets and 3 Public Subnets</li> <li>Internet gateway for Public Subnets and NAT Gateway for Private Subnets</li> <li>EKS Cluster Control plane with one managed node group</li> </ul>"},{"location":"helpers/new-eks-cluster/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>Make sure to complete the prerequisites section before proceeding.</p>"},{"location":"helpers/new-eks-cluster/#setup","title":"Setup","text":""},{"location":"helpers/new-eks-cluster/#1-download-sources-and-initialize-terraform","title":"1. Download sources and initialize Terraform","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\ncd examples/eks-cluster-with-vpc/\nterraform init\n</code></pre>"},{"location":"helpers/new-eks-cluster/#2-aws-region","title":"2. AWS Region","text":"<p>Specify the AWS Region where the resources will be deployed:</p> <pre><code>export TF_VAR_aws_region=xxx\n</code></pre>"},{"location":"helpers/new-eks-cluster/#3-cluster-name","title":"3. Cluster Name","text":"<p>Specify the name of your EKS cluster:</p> <pre><code>export TF_VAR_cluster_name=xxx\n</code></pre>"},{"location":"helpers/new-eks-cluster/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy the example</p> <pre><code>terraform apply\n</code></pre>"},{"location":"helpers/new-eks-cluster/#additional-configuration-optional","title":"Additional configuration (optional)","text":""},{"location":"helpers/new-eks-cluster/#1-instance-type","title":"1. Instance Type","text":"<p>Depending on your region or limitations in your account, you might need to change to a different instance type. To do this, you can define the instance type to use: <pre><code>export TF_VAR_managed_node_instance_type=xxx\n</code></pre></p>"},{"location":"helpers/new-eks-cluster/#2-amazon-elastic-kubernetes-service-amazon-eks-version","title":"2. Amazon Elastic Kubernetes Service (Amazon EKS) Version","text":"<p>You can override the version of the cluster also: <pre><code>export TF_VAR_eks_version=xxx\n</code></pre></p>"},{"location":"helpers/new-eks-cluster/#login-to-your-cluster","title":"Login to your cluster","text":"<p>EKS Cluster details can be extracted from terraform output or from AWS Console to get the name of cluster. Use the following commands in your local machine where you want to interact with your EKS Cluster.</p>"},{"location":"helpers/new-eks-cluster/#1-run-update-kubeconfig-command","title":"1. Run <code>update-kubeconfig</code> command","text":"<p><code>~/.kube/config</code> file gets updated with cluster details and certificate from the below command</p> <pre><code>aws eks --region &lt;enter-your-region&gt; update-kubeconfig --name &lt;cluster-name&gt;\n</code></pre>"},{"location":"helpers/new-eks-cluster/#2-list-all-the-worker-nodes-by-running-the-command-below","title":"2. List all the worker nodes by running the command below","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"helpers/new-eks-cluster/#3-list-all-the-pods-running-in-kube-system-namespace","title":"3. List all the pods running in <code>kube-system</code> namespace","text":"<pre><code>kubectl get pods -n kube-system\n</code></pre>"},{"location":"helpers/new-eks-cluster/#cleanup","title":"Cleanup","text":"<p>To clean up your environment, destroy the Terraform modules in reverse order.</p> <p>Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC</p> <pre><code>terraform destroy -target=\"module.eks_blueprints_kubernetes_addons\" -auto-approve\nterraform destroy -target=\"module.eks_blueprints\" -auto-approve\nterraform destroy -target=\"module.vpc\" -auto-approve\n</code></pre> <p>Finally, destroy any additional resources that are not in the above modules</p> <pre><code>terraform destroy -auto-approve\n</code></pre>"},{"location":"workloads/managed-prometheus/","title":"Monitoring Amazon Managed Service for Prometheus workspaces","text":"<p>This example allows you to monitor your Amazon Managed Service for Prometheus workspaces using Amazon CloudWatch vended metrics and logs. It also creates configurable CloudWatch alarms for service usage limits. Those informations are displayed in a Managed Grafana workspace dashboard.</p>"},{"location":"workloads/managed-prometheus/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>Make sure to complete the prerequisites section before proceeding. This example doesn't require an Amazon EKS cluster and Kubernetes tools (ex. <code>kubectl</code>).</p> <p>Note</p> <p>This example requires CloudWatch Billing Metrics to be enabled in order to create some of the alarms in this module.</p>"},{"location":"workloads/managed-prometheus/#setup","title":"Setup","text":""},{"location":"workloads/managed-prometheus/#1-download-sources-and-initialize-terraform","title":"1. Download sources and initialize Terraform","text":"<pre><code>git clone https://github.com/aws-observability/terraform-aws-observability-accelerator.git\ncd examples/managed-prometheus-monitoring\nterraform init\n</code></pre>"},{"location":"workloads/managed-prometheus/#2-aws-region","title":"2. AWS Region","text":"<p>Specify the AWS Region where the resources will be deployed:</p> <pre><code>export TF_VAR_aws_region=xxx\n</code></pre>"},{"location":"workloads/managed-prometheus/#3-amazon-managed-service-for-prometheus-workspace","title":"3. Amazon Managed Service for Prometheus workspace","text":"<p>Specify one or more workspaces in the same Region separated with a comma seperated string.</p> <pre><code>export TF_VAR_managed_prometheus_workspace_id=\"ws-xxx\"\n</code></pre> <p>You can use the following command to create alarms for all of the workspaces in a region.</p> <pre><code>export TF_VAR_managed_prometheus_workspace_id=$(aws amp list-workspaces --query 'workspaces[].workspaceId' --output text |  sed -E 's/\\t/,/g')\n</code></pre>"},{"location":"workloads/managed-prometheus/#4-amazon-managed-grafana-workspace","title":"4. Amazon Managed Grafana workspace","text":"<p>To run this example you need an Amazon Managed Grafana workspace.</p> <pre><code>export TF_VAR_managed_grafana_workspace_id=g-xxx\n</code></pre>"},{"location":"workloads/managed-prometheus/#5-grafana-api-key","title":"5. Grafana API Key","text":"<p>Amazon Managed Grafana provides a control plane API for generating Grafana API keys. As a security best practice, we will provide to Terraform a short lived API key to run the <code>apply</code> or <code>destroy</code> command.</p> <p>Ensure you have necessary IAM permissions (<code>CreateWorkspaceApiKey, DeleteWorkspaceApiKey</code>)</p> <pre><code>export TF_VAR_grafana_api_key=`aws grafana create-workspace-api-key --key-name \"observability-accelerator-$(date +%s)\" --key-role ADMIN --seconds-to-live 1200 --workspace-id $TF_VAR_managed_grafana_workspace_id --query key --output text`\n</code></pre>"},{"location":"workloads/managed-prometheus/#deploy","title":"Deploy","text":"<p>Simply run this command to deploy the example</p> <pre><code>terraform apply\n</code></pre>"},{"location":"workloads/managed-prometheus/#visualization","title":"Visualization","text":""},{"location":"workloads/managed-prometheus/#1-cloudwatch-datasource-on-grafana","title":"1. Cloudwatch datasource on Grafana","text":"<p>Open your Grafana workspace and under Configuration -&gt; Data sources, you should see <code>aws-observability-accelerator-cloudwatch</code>. Open and click <code>Save &amp; test</code>. You should see a notification confirming that the CloudWatch datasource is ready to be used on Grafana.</p>"},{"location":"workloads/managed-prometheus/#2-grafana-dashboards","title":"2. Grafana dashboards","text":"<p>Go to the Dashboards panel of your Grafana workspace. You should see a list of dashboards under the <code>AMP Monitoring Dashboards</code> folder.</p> <p>Open the <code>AMP Accelerator Dashboard</code> to see a visualization of the AMP workspace.</p> <p></p>"},{"location":"workloads/managed-prometheus/#3-amazon-managed-service-for-prometheus-cloudwatch-alarms","title":"3. Amazon Managed Service for Prometheus CloudWatch Alarms.","text":"<p>Open the CloudWatch console and click <code>Alarms</code> &gt; <code>All Alarms</code> to review the service limit alarms.</p> <p></p> <p>In us-east-1 region an alarm is created for billing. This alarm utilizes anomaly detection to detect anomalies in the Estimated Charges billing metric.</p> <p></p>"},{"location":"workloads/managed-prometheus/#destroy-resources","title":"Destroy resources","text":"<p>If you leave this stack running, you will continue to incur charges. To remove all resources created by Terraform, refresh your Grafana API key and run the command below.</p> <p>Warning</p> <p>Be careful, this command will remove everything created by Terraform. If you wish to keep your Amazon Managed Grafana Dashboards or CloudWatch Alarms. Remove them from your terraform state before running the destroy command.</p> <pre><code>terraform destroy\n</code></pre>"}]}